tf.nn.softmax()                 softmax 层
tf.equal(x,y)                   对应相等为 True
tf.nn.sigmoid(x,name = None)    sigmoid 函数,适用于特征差异比较小的数据
tf.nn.tanh(x,name = None)       tanh 函数,适用于特征相差比较大的数据
tf.nn.relu(x,name = None)       relu 函数，其处理后的数据具有更好的稀疏性，用以替代 sigmoid 函数
 
由于 relu 函数对负值直接过滤了，所以导致很容易使模型输出全零导致无法训练。
因此 relu 函数存在一些变种，对负值不再直接虑除，而是加以处理
tf.nn.elu(x,name = None)       e^x - 1 (x<0)
tf.nn.relu6(x,name = None)      以6 为阈值的relu ，防止梯度爆炸
swith 函数是谷歌公司发现的一个效果更优于 relu 的函数，经过测试，模型准确率更高
def Swith(x,beta = 1):
    return x * tf.nn.sigmoid(x * beta)
----------------------------------------------------------------------------------------
交叉熵: -[plnq + (1-p)ln(1-q)]
tf.nn.sigmoid_cross_entropy_with_logits(logits,targets)   计算 logits 和 targets 之间的交叉熵
tf.nn.softmax_cross_entropy_with_logits(logits,labels)    计算 softmax 交叉熵，传入的 logits 不需要进行 softmax
                                                          否则就相当于计算了两次 softmax 转换
tf.nn.sparse_softmax_cross_entropy_with_logits(logits,labels) 同上，只是不需要 one_hot 编码，[2,1] 相当于 [[0,0,1],[0,1,0]]
梯度下降分类：
批量梯度下降：遍历全部数据集算一次损失函数，然后算函数对各个参数的梯度和更新梯度。很慢
随机梯度下降：每看一个数据就计算一次损失函数，然后求梯度更新参数，很快，但是收敛性不好
小批量梯度下降：也就是最常用的计算一个batch 的损失函数，再梯度下降
tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)
tf.train.AdamOptimizer(learn_rate).minimize(loss) 反向传播算法
----------------------------------------------------------------------------------------
初始化函数：
tf.constant_initializer(value,dtype)    初始化常量
tf.random_normal_initializer(mean = 0.0,stddev = 1.0) 正太随机分布
tf.truncated_normal_initializer(mean = 0.0,stddev = 1.0) 截断的正太分布
tf.random_uniform_initializer(minval = 0,maxval = None)  均匀分布
tf.zeros_initializer(shape)  tf.ones_initializer(shape)
一般常用的初始化函数为 tf.truncated_normal_initializer 函数，因为有截断的功能，生成相对温和的初始值
另外，还有个 tf.contrib.layers.xavier_initializer 初始化函数，用来在所有层中保持梯度大体相同
Maxout 神经网络：
取一层神经网络中最敏感的节点传导下一层，Maxout 神经网络具有很强的拟合性
z = tf.matmul(x,W) + b
maxout = tf.reduce_max(z,axis = 1,keep_dims = True)
W2 = tf.Variable(tf.truncated_normal([1,10],stddev = 0.1))
b2 = tf.Variable(tf.zeros([10]))
pred = tf.nn.softmax(tf.matmul(maxout,W2) + b2)
...
