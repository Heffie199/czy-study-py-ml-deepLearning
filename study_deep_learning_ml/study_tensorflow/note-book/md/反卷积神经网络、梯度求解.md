反卷积神经网络： 卷积神经网络的可视化
反卷积过程并不具有学习的能力，仅仅是用于可视化一个已经训练好的卷积神经网络模型。
 
反卷积步骤：
1.卷积核反转（上下左右颠倒）
2.将卷积结果作为输入，做步长补零的操作。
3.在补充后的输入基础上整体补零，以使得与反卷积核卷积后的结果为原始的 shape
4.将得到的补零的结果与反卷积核进行步长为 1 的卷积，得到最终的反卷积结果。
 
tf.conv2d_transpose(value,  # 卷积后的张量
                    filter, # 卷积核 - 不用自己转置
                    output_shape, #输出的张量形状 4 维
                    strides, # 卷积时的 strides
                    padding, # 卷积时的 padding
                    name = None)
                    
注意反卷积只能恢复部分特征，并不能完全还原。
 
反池化步骤：
反平均池化:直接还原成原来的大小，然后将池化结果中每个值填入对应区域。
反最大池化:要求在池化过程中记录最大激活位置，然后把池化结果填入对应位置，其他补零。
所以在反最大池化时，必须记录最大池化激活位置
_,mask = tf.nn.max_pool_with_argmax(net,ksize = ksize,strides = strides,padding = 'SAME')
mask = tf.stop_gradient(mask) # mask 就会记录最大激活位置
net = tf.nn.max_pool(net,ksize = ksize,strides = strides,padding = 'SAME') # 最大池化层
 
-------------------------------------------------------------------------------------------
求梯度函数 tf.gradients
W1 = tf.Variable([[1,2]])
y = tf.matmul(W1,[[9],[10]])
grads = tf.gradients(y,W1)
# 求 W1 的梯度,由于 y 是由 W1 和 [[9],[10]] 相乘而来，所以其导数也就是 [[9,10]]
with tf.Session() as sess:
    tf.all_variables_initializer().run()
    gradval = sess.run(grads)
    print gradval # 输出 [array([[9,10]])]
 
使用 gradients 对多个式子求多变量偏导
W1 = tf.get_variable("W1",shape = [2])
W2 = tf.get_variable("W2",shape = [2])
W3 = tf.get_variable("W3",shape = [2])
W4 = tf.get_variable("W4",shape = [2])
y1 = W1 + W2 +W3
y2 = W3 + W4
# grad_ys 求梯度的输入值
gradients = tf.gradients([y1,y2],[W1,W2,W3,W4],grad_ys = [
                                   tf.convert_to_tensor([1.,2.]),
                                   tf.convert_to_tensor([3.,4.])])
grads_ys 给定公式结果的值，相当于 y1 = [1.,2.] y2 = [3.,4.] 时，W1,W2，W3，W4 的导数
对 W1 求导时，W2，W3 为常数，所以 W1 的导数为[1.,2.], W2,W3 也如此
同理，W3 W4 的导数为 [3.,4.]
 
梯度停止的用法，被 tf.stop_gradients 定义过的节点将没有梯度运算功能
a = W1 + W2
a_stop = tf.stop_gradients(a) # 令 a 梯度停止
y3 = a_stop + W3
gradients2 = tf.gradients(y3,[W1,W2,W3],grad_ys = tf.convert_to_tensor([1.,2.]))
运行 sess.run(gradients2) 则会报错，因为 W1 和 W2 的梯度不可求，除非去掉 W1，W2
