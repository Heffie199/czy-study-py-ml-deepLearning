tf.nn.softmax()                 softmax 层
tf.equal(x,y)                   对应相等为 True
tf.nn.sigmoid(x,name = None)    sigmoid 函数,适用于特征差异比较小的数据
tf.nn.tanh(x,name = None)       tanh 函数,适用于特征相差比较大的数据
tf.nn.relu(x,name = None)       relu 函数，其处理后的数据具有更好的稀疏性，用以替代 sigmoid 函数
 
由于 relu 函数对负值直接过滤了，所以导致很容易使模型输出全零导致无法训练。
因此 relu 函数存在一些变种，对负值不再直接虑除，而是加以处理
tf.nn.elu(x,name = None)       e^x - 1 (x<0)
 
tf.nn.relu6(x,name = None)      以6 为阈值的relu ，防止梯度爆炸
 
swith 函数是谷歌公司发现的一个效果更优于 relu 的函数，经过测试，模型准确率更高
def Swith(x,beta = 1):
    return x * tf.nn.sigmoid(x * beta)
 
隐藏层可以看做逻辑门，也可以看做将低维可分转化成高一维的可分
模型正则化有两种
tf.contrib.layers.l2_regularizer(lambda)(w)
tf.nn.l2_loss(w) * lambda
 
dropout 随机失活
layer = tf.nn.dropout(layer,keep_prob)
