退化学习率
tf.train.exponential_decay(learning_rate,global_step,train_step,0.9)
 
在搭建网络结构时，一般倾向于使用更深的模型，来减少网络中需要神经元的数量，使得网络具有更好的泛化能力。
但注意使用 sigmoid 或 tanH 激活函数时，其反向传播的有效层数大概 4 ~6 层，层数再多可能导致梯度消失无法训练的问题。
 
卷积神经网络 CNN -> 解决参数过多的问题
一般对一幅图片（或者输入矩阵）会使用多个卷积核，将它们统一放到卷积层操作，得到更深的矩阵。
一般CNN 结构如下：
输入层：将每个像素代表一个特征节点输入进来
卷积操作部分：由多个滤波器组合的卷积层
池化层：减少参数
全剧平均池化层：对生成的结果取全局平均值
输出层：softmax
 
卷积操作：
1.窄卷积：就是不使用 PADDING 填充，步长不固定
2.同卷积：使用 PADDING 填充，得到的尺寸不变，步长固定为 1
3.全卷积：也叫反卷积，把图片中的每个像素都用卷积操作展开，得到的结果比原始图片大，步长固定为 1
 
tf.nn.conv2d(input,filter,strides,padding,name = None) padding = 'SAME' 则填充到滤波器可达到图像边缘,先在右下补零
 
池化层：tf.nn.max_pool (avg_pool)
tf.nn.max_pool(input,ksize,strides,padding,name = None)
ksize: 池化窗口的大小，一般是 [1,height,width,1] 因为我们不想在 batch 和 channels 上池化
strides: 和卷积参数含义一样，一般也是 [1,stride,stride,1]
padding: 和卷积层含义一样
注意平均池化层计算的是非零值的平均值
 
-------------------------------------------------------------------------------------
在 TensorFlow 中使用 queue 多线程数据读取机制
TensorFlow 提供了一个队列机制，通过多线程将读取数据和计算数据分开，因为在处理海量数据时，无法将数据集一次性载入内存
images_test,labels_test = cifar10_input.inputs(eval_data = False,data_dir = data_dir,batch_size = 128)
sess = tf.InteractiveSession()
tf.global_variables_initializer().run()
tf.train.start_queue_runners()
image_batch,label_batch = sess.run([images_test,labels_test]) # 从队列里拿出指定批次的数据
...
如果注释掉 tf.global_variables_initializer().run() ,
则程序会在 image_batch,label_batch = sess.run([images_test,labels_test]) 挂起不动
缺点是不能 with tf.Session() 语法，因为session 一旦关闭，等待队列出错
 
协调器： tf.train.Coordinator，通常搭配 tf.train.start_queue_runners() 一起使用
coord.request_stop() 会通知数据输入线程退出
with tf.Session() as sess:
    tf.global_variables_initializer().run()
    # 定义协调器
    coord = tf.train.Coordinator()
    threads = tf.train.start_queue_runners(sess,coord)
    image_batch,label_batch = sess.run([images_test,labels_test])
    ...
    coord.request_stop()
